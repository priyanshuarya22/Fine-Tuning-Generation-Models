{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Templating Instruction Data","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom datasets import load_dataset\n\n# Load a tokenizer to use its chat template\ntemplate_tokenizer = AutoTokenizer.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n)\n\ndef format_prompt(example):\n    \"\"\"Format the prompt to using the <|user|> tempalte TinyLLama is using\"\"\"\n    \n    # Format answers\n    chat = example[\"messages\"]\n    prompt = template_tokenizer.apply_chat_template(chat, tokenize=False)\n    \n    return {\"text\": prompt}\n\n# Load and format the data using the template TinyLLama is using\ndataset = (\n    load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"test_sft\")\n    .shuffle(seed=42)\n    .select(range(3_000))\n)\ndataset = dataset.map(format_prompt)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T10:40:26.554293Z","iopub.execute_input":"2024-11-01T10:40:26.554988Z","iopub.status.idle":"2024-11-01T10:40:31.309292Z","shell.execute_reply.started":"2024-11-01T10:40:26.554939Z","shell.execute_reply":"2024-11-01T10:40:31.308251Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Example of formatted prompt\nprint(dataset[\"text\"][2576])","metadata":{"execution":{"iopub.status.busy":"2024-11-01T10:40:35.805593Z","iopub.execute_input":"2024-11-01T10:40:35.806173Z","iopub.status.idle":"2024-11-01T10:40:35.836486Z","shell.execute_reply.started":"2024-11-01T10:40:35.806132Z","shell.execute_reply":"2024-11-01T10:40:35.835397Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"<|user|>\nGiven the text: Knock, knock. Who’s there? Hike.\nCan you continue the joke based on the given text material \"Knock, knock. Who’s there? Hike\"?</s>\n<|assistant|>\nSure! Knock, knock. Who's there? Hike. Hike who? Hike up your pants, it's cold outside!</s>\n<|user|>\nCan you tell me another knock-knock joke based on the same text material \"Knock, knock. Who's there? Hike\"?</s>\n<|assistant|>\nOf course! Knock, knock. Who's there? Hike. Hike who? Hike your way over here and let's go for a walk!</s>\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Model Quantization","metadata":{}},{"cell_type":"code","source":"!pip install bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-11-01T10:40:40.757277Z","iopub.execute_input":"2024-11-01T10:40:40.757654Z","iopub.status.idle":"2024-11-01T10:40:52.293874Z","shell.execute_reply.started":"2024-11-01T10:40:40.757614Z","shell.execute_reply":"2024-11-01T10:40:52.292859Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.44.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nmodel_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n\n# 4-bit quantization configuration - Q in QLoRA\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, # Use 4-bit precision model loading\n    bnb_4bit_quant_type=\"nf4\", # Quantization type\n    bnb_4bit_compute_dtype=\"float16\", # Compute dtype\n    bnb_4bit_use_double_quant=True, # Apply nested quantization\n)\n\n# Load the model to train on the GPU\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    \n    # Leave this out for regular SFT\n    quantization_config=bnb_config,\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = \"<PAD>\"\ntokenizer.padding_side = \"left\"","metadata":{"execution":{"iopub.status.busy":"2024-11-01T10:40:57.278984Z","iopub.execute_input":"2024-11-01T10:40:57.279422Z","iopub.status.idle":"2024-11-01T10:41:00.629478Z","shell.execute_reply.started":"2024-11-01T10:40:57.279379Z","shell.execute_reply":"2024-11-01T10:41:00.628638Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"LoRA Configuration","metadata":{}},{"cell_type":"code","source":"!pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-11-01T10:41:07.126213Z","iopub.execute_input":"2024-11-01T10:41:07.126814Z","iopub.status.idle":"2024-11-01T10:41:18.790965Z","shell.execute_reply.started":"2024-11-01T10:41:07.126759Z","shell.execute_reply":"2024-11-01T10:41:18.789712Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.13.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.34.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n\n# Prepare LoRA Configuration\npeft_config = LoraConfig(\n    lora_alpha=32, # LoRA Scaling\n    lora_dropout=0.1, # Dropout for LoRA Layers\n    r=64, # Rank\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules= # Layers to target\n    [\"k_proj\", \"gate_proj\", \"v_proj\", \"up_proj\", \"q_proj\", \"o_proj\", \"down_proj\"]\n)\n\n# Prepare model for training\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T10:41:24.287769Z","iopub.execute_input":"2024-11-01T10:41:24.288748Z","iopub.status.idle":"2024-11-01T10:41:25.188525Z","shell.execute_reply.started":"2024-11-01T10:41:24.288701Z","shell.execute_reply":"2024-11-01T10:41:25.187642Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Training Configuration","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\noutput_dir = \"./results\"\n\n# Training arguments\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    optim=\"paged_adamw_32bit\",\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    num_train_epochs=1,\n    logging_steps=10,\n    fp16=True,\n    gradient_checkpointing=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T10:41:29.272208Z","iopub.execute_input":"2024-11-01T10:41:29.272631Z","iopub.status.idle":"2024-11-01T10:41:32.696097Z","shell.execute_reply.started":"2024-11-01T10:41:29.272585Z","shell.execute_reply":"2024-11-01T10:41:32.695074Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Training","metadata":{}},{"cell_type":"code","source":"!pip install trl","metadata":{"execution":{"iopub.status.busy":"2024-11-01T10:41:37.328322Z","iopub.execute_input":"2024-11-01T10:41:37.329508Z","iopub.status.idle":"2024-11-01T10:41:48.974356Z","shell.execute_reply.started":"2024-11-01T10:41:37.329464Z","shell.execute_reply":"2024-11-01T10:41:48.973084Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: trl in /opt/conda/lib/python3.10/site-packages (0.11.4)\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl) (2.4.0)\nRequirement already satisfied: transformers>=4.40.0 in /opt/conda/lib/python3.10/site-packages (from trl) (4.45.1)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl) (0.34.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl) (3.0.1)\nRequirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl) (0.8.14)\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (2024.6.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (0.25.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (4.66.4)\nRequirement already satisfied: docstring-parser>=0.16 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (0.16)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.7.1)\nRequirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (1.7.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl) (5.9.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.9.5)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.40.0->trl) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.40.0->trl) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.40.0->trl) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.40.0->trl) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.40.0->trl) (2024.8.30)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.18.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!wandb offline","metadata":{"execution":{"iopub.status.busy":"2024-11-01T10:41:54.846967Z","iopub.execute_input":"2024-11-01T10:41:54.847414Z","iopub.status.idle":"2024-11-01T10:41:56.769444Z","shell.execute_reply.started":"2024-11-01T10:41:54.847370Z","shell.execute_reply":"2024-11-01T10:41:56.768188Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"W&B offline. Running your script from this directory will only write metadata locally. Use wandb disabled to completely turn off W&B.\n","output_type":"stream"}]},{"cell_type":"code","source":"from trl import SFTTrainer\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    max_seq_length=512,\n    # Leave this out for regular SFT\n    peft_config=peft_config,\n)\n\n# Train model\ntrainer.train()\n\n# Save QLoRA weights\ntrainer.model.save_pretrained(\"TinyLlama-1.1B-qlora\")","metadata":{"execution":{"iopub.status.busy":"2024-11-01T10:42:00.678934Z","iopub.execute_input":"2024-11-01T10:42:00.679963Z","iopub.status.idle":"2024-11-01T11:05:21.409139Z","shell.execute_reply.started":"2024-11-01T10:42:00.679915Z","shell.execute_reply":"2024-11-01T11:05:21.408113Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [375/375 23:11, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.669100</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.475700</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.451100</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.488000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.477800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.390500</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.494600</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.449900</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.427300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.404300</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.414000</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.376900</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.332000</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.496700</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.345900</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.411800</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.454200</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.324800</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.419700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.474200</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.404000</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.342200</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>1.361200</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.386900</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.354100</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.345900</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>1.465500</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.434100</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>1.387100</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.376200</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>1.395200</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>1.438000</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>1.387200</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>1.388000</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.313200</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.444400</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>1.451800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"from peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    \"TinyLlama-1.1B-qlora\",\n    low_cpu_mem_usage=True,\n    device_map=\"auto\",\n)\n\n# Merge LoRA and base model\nmerged_model = model.merge_and_unload()","metadata":{"execution":{"iopub.status.busy":"2024-11-01T11:09:07.867448Z","iopub.execute_input":"2024-11-01T11:09:07.868498Z","iopub.status.idle":"2024-11-01T11:09:10.726882Z","shell.execute_reply.started":"2024-11-01T11:09:07.868439Z","shell.execute_reply":"2024-11-01T11:09:10.725781Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\n# Use our predefined prompt template\nprompt = \"\"\"<|user|>\nTell me something about Large Language Models.</s>\n<|assistant|>\n\"\"\"\n\n# Run our instruction-tuned model\npipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer)\nprint(pipe(prompt)[0][\"generated_text\"])","metadata":{"execution":{"iopub.status.busy":"2024-11-01T11:11:51.312190Z","iopub.execute_input":"2024-11-01T11:11:51.312881Z","iopub.status.idle":"2024-11-01T11:12:03.035993Z","shell.execute_reply.started":"2024-11-01T11:11:51.312838Z","shell.execute_reply":"2024-11-01T11:12:03.035056Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"<|user|>\nTell me something about Large Language Models.</s>\n<|assistant|>\nLarge Language Models (LLMs) are a type of artificial intelligence (AI) that can generate human-like language. They are trained on large amounts of data, including text, audio, and video, and are capable of generating complex sentences and phrases that are often difficult to create by humans.\n\nLLMs are used in a variety of applications, including natural language processing (NLP), machine translation, and chatbots. They can be used to generate text in different languages, such as English, French, or Chinese, and can also be used to generate images, videos, or other forms of content.\n\nOne of the most significant applications of LLMs is in the field of natural language generation (NLG). LLMs can be used to generate text that is human-like, such as news articles, blog posts, or social media posts. They can also be used to generate dialogue systems, such as chatbots, that can respond to user queries in a natural and conversational manner.\n\nLLMs have also been used in the field of machine translation, where they can be trained to translate between different languages. This has been particularly useful in areas where there is a need for rapid translation, such as in the healthcare industry or in international diplomacy.\n\nOverall, LLMs are a powerful tool that has the potential to revolutionize the way we communicate and interact with each other. They are being used in a wide range of applications, and their capabilities are only expected to grow in the future.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Templating Alignment Data","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndef format_prompt(example):\n    \"\"\"Format the prompt to using the <|user|> template TinyLLama is using\"\"\"\n    \n    # Format answers\n    system = \"<|system|>\\n\" + example[\"system\"] + \"</s>\\n\"\n    prompt = \"<|user|>\\n\" + example[\"input\"] + \"</s>\\n<|assistant|>\\n\"\n    chosen = example[\"chosen\"] + \"</s>\\n\"\n    rejected = example[\"rejected\"] + \"</s>\\n\"\n    return {\n        \"prompt\": system + prompt,\n        \"chosen\": chosen,\n        \"rejected\": rejected,\n    }\n\n# Apply formatting to the dataset and select relatively short answers\ndpo_dataset = load_dataset(\n    \"argilla/distilabel-intel-orca-dpo-pairs\",\n    split=\"train\"\n)\ndpo_dataset = dpo_dataset.filter(\n    lambda r:\n        r[\"status\"] != \"tie\" and\n        r[\"chosen_score\"] >= 8 and\n        not r[\"in_gsm8k_train\"]\n)\ndpo_dataset = dpo_dataset.map(\n    format_prompt, remove_columns=dpo_dataset.column_names\n)\ndpo_dataset","metadata":{"execution":{"iopub.status.busy":"2024-11-01T11:39:58.088618Z","iopub.execute_input":"2024-11-01T11:39:58.089292Z","iopub.status.idle":"2024-11-01T11:40:05.179701Z","shell.execute_reply.started":"2024-11-01T11:39:58.089249Z","shell.execute_reply":"2024-11-01T11:40:05.178545Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"560b8bb1addc42f29e90107fa31dcdfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/79.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25020a568d1148b5aee6b9ca65afd22b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/12859 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a6c59a0be9a4369bab8cb0a5b92ac09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/12859 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"661a78b33ee645ee9c691ded324dfe9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5922 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b977bb8c72ba415d95cbe1abdd3b0959"}},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['chosen', 'rejected', 'prompt'],\n    num_rows: 5922\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"Model Quantization","metadata":{}},{"cell_type":"code","source":"from peft import AutoPeftModelForCausalLM\nfrom transformers import BitsAndBytesConfig, AutoTokenizer\n\n# 4-bit quantization configuration - Q in QLoRA\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, # Use 4-bit precision model loading\n    bnb_4bit_quant_type=\"nf4\", # Quantization type\n    bnb_4bit_compute_dtype=\"float16\", # Compute dtype\n    bnb_4bit_use_double_quant=True, # Apply nested quantization\n)\n\n# Merge LoRA and base model\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    \"TinyLlama-1.1B-qlora\",\n    low_cpu_mem_usage=True,\n    device_map=\"auto\",\n    quantization_config=bnb_config,\n)\nmerged_model = model.merge_and_unload()\n\n# Load LLaMA tokenizer\nmodel_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = \"<PAD>\"\ntokenizer.padding_side = \"left\"","metadata":{"execution":{"iopub.status.busy":"2024-11-01T11:46:42.016311Z","iopub.execute_input":"2024-11-01T11:46:42.016779Z","iopub.status.idle":"2024-11-01T11:46:50.212666Z","shell.execute_reply.started":"2024-11-01T11:46:42.016737Z","shell.execute_reply":"2024-11-01T11:46:50.211625Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:336: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n\n# Prepare LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=32, # LoRA Scaling\n    lora_dropout=0.1, # Dropout for LoRA Layers\n    r=64, # Rank\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules= # Layers to target\n     [\"k_proj\", \"gate_proj\", \"v_proj\", \"up_proj\", \"q_proj\", \"o_proj\", \"down_proj\"]\n)\n\n# prepare model for training\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T11:50:21.975752Z","iopub.execute_input":"2024-11-01T11:50:21.976672Z","iopub.status.idle":"2024-11-01T11:50:22.750992Z","shell.execute_reply.started":"2024-11-01T11:50:21.976628Z","shell.execute_reply":"2024-11-01T11:50:22.748149Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Training Configuration","metadata":{}},{"cell_type":"code","source":"from trl import DPOConfig\n\noutput_dir = \"./results\"\n\n# Training arguments\ntraining_arguments = DPOConfig(\n    output_dir=output_dir,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    optim=\"paged_adamw_32bit\",\n    learning_rate=1e-5,\n    lr_scheduler_type=\"cosine\",\n    max_steps=200,\n    logging_steps=10,\n    fp16=True,\n    gradient_checkpointing=True,\n    warmup_ratio=0.1\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T11:53:53.131318Z","iopub.execute_input":"2024-11-01T11:53:53.132100Z","iopub.status.idle":"2024-11-01T11:53:53.176516Z","shell.execute_reply.started":"2024-11-01T11:53:53.132057Z","shell.execute_reply":"2024-11-01T11:53:53.175630Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Training","metadata":{}},{"cell_type":"code","source":"from trl import DPOTrainer\n\n# Create DPO trainer\ndpo_trainer = DPOTrainer(\n    model,\n    args=training_arguments,\n    train_dataset=dpo_dataset,\n    tokenizer=tokenizer,\n    peft_config=peft_config,\n    beta=0.1,\n    max_prompt_length=512,\n    max_length=512,\n)\n\n# Fine-tune model with DPO\ndpo_trainer.train()\n\n# Save adapter\ndpo_trainer.model.save_pretrained(\"TinyLlama-1.1B-dpo-qlora\")","metadata":{"execution":{"iopub.status.busy":"2024-11-01T11:57:48.281808Z","iopub.execute_input":"2024-11-01T11:57:48.282248Z","iopub.status.idle":"2024-11-01T12:18:42.418433Z","shell.execute_reply.started":"2024-11-01T11:57:48.282206Z","shell.execute_reply":"2024-11-01T12:18:42.417495Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/5922 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a00f88a698f4424d88a8968240d9ad78"}},"metadata":{}},{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\nCould not estimate the number of tokens of the input, floating-point operations will not be computed\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 20:22, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.692900</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.678400</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.645900</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.606700</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.595400</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.616800</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.593000</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.531900</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.559100</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.639200</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.496600</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.585700</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.630700</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.591000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.578500</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.591600</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.607100</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.626800</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.669500</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.555200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"from peft import PeftModel\n\n# Merge LoRA and base model\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    \"TinyLlama-1.1B-qlora\",\n    low_cpu_mem_usage=True,\n    device_map=\"auto\",\n)\nsft_model = model.merge_and_unload()\n\n# Merge DPO LoRA and SFT model\ndpo_model = PeftModel.from_pretrained(\n    sft_model,\n    \"TinyLlama-1.1B-dpo-qlora\",\n    device_map=\"auto\",\n)\ndpo_model = dpo_model.merge_and_unload()","metadata":{"execution":{"iopub.status.busy":"2024-11-01T12:21:43.738404Z","iopub.execute_input":"2024-11-01T12:21:43.739122Z","iopub.status.idle":"2024-11-01T12:21:48.647032Z","shell.execute_reply.started":"2024-11-01T12:21:43.739078Z","shell.execute_reply":"2024-11-01T12:21:48.646251Z"},"trusted":true},"execution_count":20,"outputs":[]}]}